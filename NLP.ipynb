{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlgqVvG598c0",
        "outputId": "14dd36e3-60b0-4a1e-b2f8-f698c3267b27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization:\n",
        "tokenization means breaking a sentence in to words\n",
        "\n",
        "# example:\n",
        "\n",
        "\"i love programming\"----\"i\",\"love\",\"programming\"\n",
        "\n",
        "\n",
        "we use tokenization because ML modles cannot understand read full sentences.they understand words\n"
      ],
      "metadata": {
        "id": "HfZW68UDBe23"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "sentence = \"Hello! How are you doing today?\"\n",
        "tokens = tokenizer.tokenize(sentence)\n",
        "print(tokens)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Qb7qRll_dkP",
        "outputId": "88f3ba21-b2f5-4154-95bc-a3d3214bd817"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', '!', 'How', 'are', 'you', 'doing', 'today', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "text = \"Hello! How are you doing today? I hope you are learning NLP.\"\n",
        "\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokens = tokenizer.tokenize(text)\n",
        "print(\"Tokens:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIe_0dlUG3V1",
        "outputId": "781a0149-65a4-4cee-f209-db433e00223f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens: ['Hello', '!', 'How', 'are', 'you', 'doing', 'today', '?', 'I', 'hope', 'you', 'are', 'learning', 'NLP', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stop words Removal:\n",
        "stop wors=very common words that dont add meaning\n",
        "# Example:\n",
        "\n",
        "(is,am ,are) ,(the ,a ,an)\n",
        "\n",
        "\"iam looking for a laptop\"---after stop words removal the sentence looks like\n",
        "\n",
        "\"looking\",\"laptop\""
      ],
      "metadata": {
        "id": "kznBwqehCcTa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "filtered_words = [word for word in tokens if word.lower() not in stop_words]\n",
        "print(filtered_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-VvJsX5_0M7",
        "outputId": "009ca917-b221-492d-d162-271529b57171"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', '!', 'today', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "words = \"i am a student and learning about chatbots and NLP\"\n",
        "\n",
        "\n",
        "word_list = words.split()\n",
        "\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [w for w in word_list if w.lower() not in stop_words]\n",
        "\n",
        "print(\"After stopwords removal:\", filtered_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAIx5bX3HbK3",
        "outputId": "1017ebf2-e6d5-4a37-8e87-1de9c1107c77"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After stopwords removal: ['student', 'learning', 'chatbots', 'NLP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization:\n",
        "\n",
        "lemmatization means converting words into its original basic form\n",
        "\n",
        "\"running\",\"runs\",\"ran\"-------\"run\"\n",
        "\n",
        "\n",
        "it is because \"running\",\"ran\",\"runs\"---all have one meaning \"run\""
      ],
      "metadata": {
        "id": "Nq29gq1lDX90"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word.lower()) for word in filtered_words]\n",
        "print(lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7M7Q8NvAHSk",
        "outputId": "2744e49b-e14b-4bb8-9b30-e90627738834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', '!', 'today', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag Of Words(BOW):\n",
        "this converts words into numbers\n",
        "\n",
        "example:\n",
        "\n",
        "\n",
        "i like apple---{1,1,0,1}\n",
        "\n",
        "i hate apple----{1,0,1,1}\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PY68diUlD82O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"Hello, how are you?\",\n",
        "    \"Hi there, how is it going?\",\n",
        "    \"Good morning!\"\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(X.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHcvOy_CAN_r",
        "outputId": "18175b7b-aac5-46a9-906d-aa2174bdf62a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['are' 'going' 'good' 'hello' 'hi' 'how' 'is' 'it' 'morning' 'there' 'you']\n",
            "[[1 0 0 1 0 1 0 0 0 0 1]\n",
            " [0 1 0 0 1 1 1 1 0 1 0]\n",
            " [0 0 1 0 0 0 0 0 1 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"i am a student and learning about chatbots and NLP\",\n",
        "    \"chatbots are useful in many applications\",\n",
        "    \"learning NLP is fun and interesting\"\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X_bow = vectorizer.fit_transform(corpus)\n",
        "print(\"\\nVocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(\"BoW Array:\\n\", X_bow.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-IanmkCDIyl8",
        "outputId": "281245db-f85b-410c-d395-24fcbc82060c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Vocabulary: ['about' 'am' 'and' 'applications' 'are' 'chatbots' 'fun' 'in'\n",
            " 'interesting' 'is' 'learning' 'many' 'nlp' 'student' 'useful']\n",
            "BoW Array:\n",
            " [[1 1 2 0 0 1 0 0 0 0 1 0 1 1 0]\n",
            " [0 0 0 1 1 1 0 1 0 0 0 1 0 0 1]\n",
            " [0 0 1 0 0 0 1 0 1 1 1 0 1 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TF-IDF(term frequency---inverse document frequency):\n",
        "\n",
        "it is better version of bag of words(BoW)\n",
        "\n",
        "it give importance to unique words and less importance to common words\n",
        "\n",
        "example:(in weather chatbot):\n",
        "\n",
        "\"weather\"  -- is a common words gets less importance\n",
        "\n",
        "\n",
        "\"temprature\"--is a specific word so gets more importance\n",
        "\n",
        "\n",
        "it help ml models to predict more accuartely\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "M8cBN_F-Euhl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "X_tfidf = tfidf.fit_transform(corpus)\n",
        "\n",
        "print(tfidf.get_feature_names_out())\n",
        "print(X_tfidf.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDx-8xCTAQkv",
        "outputId": "6cc4491e-493b-4781-d468-fa5ec2a19cd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['are' 'going' 'good' 'hello' 'hi' 'how' 'is' 'it' 'morning' 'there' 'you']\n",
            "[[0.52863461 0.         0.         0.52863461 0.         0.40204024\n",
            "  0.         0.         0.         0.         0.52863461]\n",
            " [0.         0.42339448 0.         0.         0.42339448 0.32200242\n",
            "  0.42339448 0.42339448 0.         0.42339448 0.        ]\n",
            " [0.         0.         0.70710678 0.         0.         0.\n",
            "  0.         0.         0.70710678 0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"i am a student and learning about chatbots and NLP\",\n",
        "    \"chatbots are useful in many applications\",\n",
        "    \"learning NLP is fun and interesting\"\n",
        "]\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
        "print(\"\\nVocabulary:\", tfidf_vectorizer.get_feature_names_out())\n",
        "print(\"TF-IDF Array:\\n\", X_tfidf.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y6l5GASlI-uJ",
        "outputId": "d5af5719-7a0a-4b75-c1d1-4aab82ad4677"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Vocabulary: ['about' 'am' 'and' 'applications' 'are' 'chatbots' 'fun' 'in'\n",
            " 'interesting' 'is' 'learning' 'many' 'nlp' 'student' 'useful']\n",
            "TF-IDF Array:\n",
            " [[0.37665395 0.37665395 0.57291007 0.         0.         0.28645504\n",
            "  0.         0.         0.         0.         0.28645504 0.\n",
            "  0.28645504 0.37665395 0.        ]\n",
            " [0.         0.         0.         0.42339448 0.42339448 0.32200242\n",
            "  0.         0.42339448 0.         0.         0.         0.42339448\n",
            "  0.         0.         0.42339448]\n",
            " [0.         0.         0.34949812 0.         0.         0.\n",
            "  0.45954803 0.         0.45954803 0.45954803 0.34949812 0.\n",
            "  0.34949812 0.         0.        ]]\n"
          ]
        }
      ]
    }
  ]
}