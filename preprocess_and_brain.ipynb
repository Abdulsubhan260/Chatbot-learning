{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# These are preprocessing steps in which we processed data and make it perfect for putting in it brain(embeddings,RNN and LSTM)\n",
        "\n",
        "\n",
        "\n",
        "in step 1.1 we are building registers meaning giving every number a special index number\n",
        "\n",
        "first two index are reserved and after them we are giving every number a\n",
        "\n",
        "special index number and if a word is repeating we are not assigning it index\n",
        "\n",
        "number repeatingly(e.g \"i love \" is repeating in sentence but we are giving them index number only one time)"
      ],
      "metadata": {
        "id": "YUbNV01weEO4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmjseclmSxmG",
        "outputId": "28568c8f-157c-49b0-d95a-f8630627f471"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word2index : {'<PAD>': 0, '<UNKN>': 1, 'i': 2, 'love': 3, 'tea': 4, 'coding': 5}\n",
            "current_index : 6\n"
          ]
        }
      ],
      "source": [
        "# for preprocessing of data to put it in RNN,Embeddings and LSTM\n",
        "\n",
        "# step 1.1 : Building Registers\n",
        "\n",
        "word2index={\n",
        "    \"<PAD>\":0,\"<UNKN>\":1\n",
        "}\n",
        "\n",
        "sentences=[[\"i\",\"love\",\"tea\"],\n",
        "           [\"i\",\"love\",\"coding\"]]\n",
        "\n",
        "current_index=2\n",
        "for sentence in sentences:\n",
        "  for word in sentence:\n",
        "    if word not in word2index:\n",
        "      word2index[word]=current_index\n",
        "      current_index=current_index+1\n",
        "\n",
        "print(f\"word2index : {word2index}\")\n",
        "\n",
        "print(f\"current_index : {current_index}\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding:\n",
        "encoding is basically used to show index numbers(IDs) of given words and if there is an unknown word which is not in register it replace it with \"UNKN\" which is already reserved in register\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "The function loops through every word in your sentence list (like \"i\", \"love\")\n",
        "and it check your Register (dictionary) to find the unique ID number for each word and\n",
        "\n",
        "If the word is not in the book (like \"babar\"), it swaps it for the special \"Unknown\" ID and\n",
        "Finally it replaces your words with their ID Numbers so the computer can read them"
      ],
      "metadata": {
        "id": "gyY-PAqgfRun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# step 1.2 : Encoding Sentences\n",
        "\n",
        "def encode_sentence(sentence,vocab_dict):\n",
        "  encoded=[]\n",
        "\n",
        "  for word in sentence:\n",
        "    if word in vocab_dict:\n",
        "      encoded.append(vocab_dict[word])\n",
        "    else:\n",
        "      encoded.append(vocab_dict[\"<UNKN>\"])\n",
        "\n",
        "  return encoded\n",
        "\n",
        "\n",
        "\n",
        "test_sentence=[\"i\",\"love\",\"coding\",\"babar\"]\n",
        "\n",
        "\n",
        "print(f\"encoded_sentence : {encode_sentence(test_sentence,word2index)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pdp-vuPZjjl",
        "outputId": "9ba905d6-3ee3-48f8-b5e6-8a931ad718ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "encoded_sentence : [2, 3, 5, 1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Padding Sentences:\n",
        "\n",
        "it means if one list has small size [1] and other has bigger size [1,2,3,4,5] it fills first list with zeros untill its length becomes equal to second/other given list\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D408rdA4iXJK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# step  1.3 : Padding Sentences(The Equalizer)\n",
        "\n",
        "my_batch=[[22],\n",
        "\n",
        " [2,3,5,1,9,8]]\n",
        "\n",
        "\n",
        "\n",
        "max_length=5\n",
        "\n",
        "def pad_seq(batch,max_len):\n",
        "  padded_batch=[]\n",
        "\n",
        "  for seq in batch:\n",
        "    current_len=len(seq)\n",
        "    if current_len<max_len:\n",
        "      diff=max_len-current_len\n",
        "      zeros=[0]*diff\n",
        "      new_seq=seq+zeros\n",
        "      padded_batch.append(new_seq)\n",
        "\n",
        "    else:\n",
        "      new_seq=seq[:max_len]\n",
        "      padded_batch.append(new_seq)\n",
        "\n",
        "\n",
        "  return padded_batch\n",
        "\n",
        "\n",
        "print(f\"pad_seq : {pad_seq(my_batch,max_length)}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fepQ6LIbhYu",
        "outputId": "1197a777-6b4f-4ca8-e576-eb02f8ad9d54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pad_seq : [[22, 0, 0, 0, 0], [2, 3, 5, 1, 9]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# the brain(RNNNet network):\n"
      ],
      "metadata": {
        "id": "6QE9Xe1e5sb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# making a brain RNNNet\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class RNNNet(nn.Module):\n",
        "  def __init__(self,vocab_size,embed_dim,hidden_size,num_classes):\n",
        "    super(RNNNet,self).__init__()\n",
        "\n",
        "\n",
        "    self.embed=nn.Embedding(vocab_size,embed_dim)\n",
        "\n",
        "\n",
        "    self.lstm=nn.LSTM(embed_dim,hidden_size,batch_first=True)\n",
        "\n",
        "\n",
        "\n",
        "    self.fc=nn.Linear(hidden_size,num_classes)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,x):\n",
        "    out=self.embed(x)\n",
        "\n",
        "    out,_=self.lstm(out)\n",
        "\n",
        "\n",
        "\n",
        "    out=out[:,-1,:]\n",
        "\n",
        "\n",
        "    out=self.fc(out)\n",
        "\n",
        "\n",
        "    return out\n",
        "\n"
      ],
      "metadata": {
        "id": "rwbaFLIELHL1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}