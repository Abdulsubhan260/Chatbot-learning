{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit\n"
      ],
      "metadata": {
        "id": "H6Qyk-ZxOUAJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1a61ea6-b065-4b6d-8aa8-1773a6810802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "import json\n",
        "import torch.nn as nn\n",
        "from transformers import BertModel, BertTokenizer\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import AdamW\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "with open('intents.json', 'r') as f:\n",
        "    intents = json.load(f)\n",
        "\n",
        "\n",
        "tags = []\n",
        "for intent in intents['intents']:\n",
        "    tags.append(intent['tag'])\n",
        "tags = sorted(set(tags))\n",
        "\n",
        "\n",
        "input_ids = []\n",
        "attention_masks = []\n",
        "labels = []\n",
        "\n",
        "\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        encoded = tokenizer(\n",
        "            pattern,\n",
        "            add_special_tokens=True,\n",
        "            max_length=20,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "\n",
        "        input_ids.append(encoded['input_ids'][0])\n",
        "        attention_masks.append(encoded['attention_mask'][0])\n",
        "\n",
        "        label_ids = tags.index(intent['tag'])\n",
        "        labels.append(label_ids)\n",
        "\n",
        "\n",
        "input_ids = torch.stack(input_ids)\n",
        "attention_masks = torch.stack(attention_masks)\n",
        "labels = torch.tensor(labels)\n",
        "\n",
        "\n",
        "class ChatDataset(Dataset):\n",
        "    def __init__(self, encodings, mask, labels):\n",
        "\n",
        "        self.encodings = encodings\n",
        "        self.mask = mask\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        return {\n",
        "            'input_ids': self.encodings[idx],\n",
        "            'attention_mask': self.mask[idx],\n",
        "            'labels': self.labels[idx]\n",
        "        }\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "\n",
        "dataset = ChatDataset(input_ids, attention_masks, labels)\n",
        "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "\n",
        "class Bert_Arch(nn.Module):\n",
        "    def __init__(self, output_dim):\n",
        "        super(Bert_Arch, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "\n",
        "        # for param in self.bert.parameters():\n",
        "        #     param.requires_grad = False\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc = nn.Linear(768, output_dim)\n",
        "\n",
        "    def forward(self, sent_id, mask):\n",
        "        output = self.bert(sent_id, attention_mask=mask)\n",
        "        cls_vector = output.pooler_output\n",
        "        x = self.fc(self.dropout(cls_vector))\n",
        "        return x\n",
        "\n",
        "output_dim = len(tags)\n",
        "model = Bert_Arch(output_dim)\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "cross_entropy = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "\n",
        "epochs = 50\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        sent_id = batch['input_ids'].to(device)\n",
        "        mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        model.zero_grad()\n",
        "\n",
        "\n",
        "        preds = model(sent_id, mask)\n",
        "\n",
        "        loss = cross_entropy(preds, labels)\n",
        "        total_loss = total_loss + loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "\n",
        "    if (epoch+1) % 20 == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "output_data = {\n",
        "    \"model_state\": model.state_dict(),\n",
        "    \"output_dim\": output_dim,\n",
        "    \"tags\": tags,\n",
        "    \"vocab_size\": len(tokenizer),\n",
        "    \"embed_dim\": 768,\n",
        "    \"hidden_size\": 768,\n",
        "    \"max_len\": 20\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "torch.save(output_data, \"bert_data.pth\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcghkmX6Op2p",
        "outputId": "d2eecef4-a248-4839-c002-81ce4ac5d662"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZiFSJW5OsE3",
        "outputId": "d8bbd26e-d86d-4b1f-bd39-1a4edecfb768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 193kB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 830kB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 775kB/s]\n",
            "config.json: 100% 570/570 [00:00<00:00, 1.90MB/s]\n",
            "model.safetensors: 100% 440M/440M [00:03<00:00, 115MB/s]\n",
            "Loading weights: 100% 199/199 [00:00<00:00, 1104.43it/s, Materializing param=pooler.dense.weight]\n",
            "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-uncased\n",
            "Key                                        | Status     |  | \n",
            "-------------------------------------------+------------+--+-\n",
            "cls.predictions.bias                       | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "cls.predictions.transform.LayerNorm.weight | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "cls.predictions.transform.dense.weight     | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "cls.predictions.transform.dense.bias       | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "cls.seq_relationship.weight                | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "cls.seq_relationship.bias                  | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "cls.predictions.transform.LayerNorm.bias   | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- \u001b[38;5;208mUNEXPECTED\u001b[0m\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
            "Epoch 20/50 | Loss: 0.4907\n",
            "Epoch 40/50 | Loss: 0.0356\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import json\n",
        "import random\n",
        "\n",
        "\n",
        "st.set_page_config(page_title=\"AI Chatbot\", page_icon=\"ü§ñ\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "class Bert_Arch(nn.Module):\n",
        "    def __init__(self, output_dim):\n",
        "        super(Bert_Arch, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.fc = nn.Linear(768, output_dim)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "\n",
        "        output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        cls_vector = output.pooler_output\n",
        "        x = self.fc(self.dropout(cls_vector))\n",
        "        return x\n",
        "\n",
        "\n",
        "@st.cache_resource\n",
        "def load_brain():\n",
        "    with open('intents.json', 'r') as f:\n",
        "        intents = json.load(f)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        data = torch.load('bert_data.pth')\n",
        "    else:\n",
        "        data = torch.load('bert_data.pth', map_location=torch.device('cpu'))\n",
        "\n",
        "    model_state = data['model_state']\n",
        "    output_dim = data['output_dim']\n",
        "    tags = data['tags']\n",
        "    max_len = data['max_len']\n",
        "\n",
        "    model = Bert_Arch(output_dim)\n",
        "    model.load_state_dict(model_state)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    return tokenizer, model, intents, tags, max_len\n",
        "\n",
        "\n",
        "if 'logged_in' not in st.session_state:\n",
        "    st.session_state.logged_in = False\n",
        "\n",
        "def login():\n",
        "    st.title(\" Login to AI Bot\")\n",
        "    username = st.text_input(\"Username\")\n",
        "    password = st.text_input(\"Password\", type=\"password\")\n",
        "\n",
        "    if st.button(\"Login\"):\n",
        "        if username == \"admin\" and password == \"123\":\n",
        "            st.session_state.logged_in = True\n",
        "            st.rerun()\n",
        "        else:\n",
        "            st.error(\"Incorrect Username or Password\")\n",
        "\n",
        "def logout():\n",
        "    st.session_state.logged_in = False\n",
        "    st.rerun()\n",
        "\n",
        "\n",
        "if not st.session_state.logged_in:\n",
        "    login()\n",
        "else:\n",
        "\n",
        "\n",
        "\n",
        "    with st.sidebar:\n",
        "        st.write(\"Welcome, User!\")\n",
        "        if st.button(\"Logout\"):\n",
        "            logout()\n",
        "\n",
        "\n",
        "    tokenizer, model, intents, tags, max_len = load_brain()\n",
        "\n",
        "\n",
        "    st.title(\"ü§ñ General Purpose AI Chatbot\")\n",
        "    st.write(\"Ask about Pakistan, Jokes, Weather, etc.\")\n",
        "\n",
        "\n",
        "    user_input = st.text_input(\"You:\")\n",
        "\n",
        "    if user_input and st.button(\"Send\"):\n",
        "\n",
        "        tokens = tokenizer(\n",
        "            user_input,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = tokens['input_ids'].to(device)\n",
        "        attention_mask = tokens['attention_mask'].to(device)\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "\n",
        "\n",
        "        prob = torch.softmax(outputs, dim=1)\n",
        "        conf, pred = torch.max(prob, dim=1)\n",
        "        tag = tags[pred.item()]\n",
        "\n",
        "        confidence_score = conf.item()\n",
        "\n",
        "        if confidence_score > 0.50:\n",
        "            found = False\n",
        "            for intent in intents['intents']:\n",
        "                if intent['tag'] == tag:\n",
        "                    response = random.choice(intent['responses'])\n",
        "                    st.success(f\"Sam: {response}\")\n",
        "                    found = True\n",
        "                    break\n",
        "            if not found:\n",
        "                st.error(\"Sam: Error finding intent in DB.\")\n",
        "        else:\n",
        "            st.error(f\"Sam: I do not understand... (Confidence: {confidence_score:.2f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9SEdOvXl55MQ",
        "outputId": "fb03772f-d0af-4149-b27e-b902584344e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py --server.port 8501 --server.enableCORS false &>/content/logs.txt &\n"
      ],
      "metadata": {
        "id": "PLUqLWFePBMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ps aux | grep streamlit\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKXDCg76PH4m",
        "outputId": "c257b90a-5968-4ca1-b39b-9e73abc8dec7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root        2262 59.8  0.5 236300 73752 ?        Sl   10:16   0:02 /usr/bin/python3 /usr/local/bin/streamlit run app.py --server.port 8501 --server.enableCORS false\n",
            "root        2288  0.0  0.0   7376  3532 ?        S    10:16   0:00 /bin/bash -c ps aux | grep streamlit\n",
            "root        2290  0.0  0.0   6484  2344 ?        S    10:16   0:00 grep streamlit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R--Ltw0FSl8c",
        "outputId": "272e02a1-60af-48e6-c9e8-94d07ddb05c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-02-03 10:16:16--  https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/cloudflare/cloudflared/releases/download/2026.1.2/cloudflared-linux-amd64 [following]\n",
            "--2026-02-03 10:16:17--  https://github.com/cloudflare/cloudflared/releases/download/2026.1.2/cloudflared-linux-amd64\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://release-assets.githubusercontent.com/github-production-release-asset/106867604/31ade8d6-2fcd-4925-9218-5534d27a01dc?sp=r&sv=2018-11-09&sr=b&spr=https&se=2026-02-03T10%3A50%3A55Z&rscd=attachment%3B+filename%3Dcloudflared-linux-amd64&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2026-02-03T09%3A50%3A17Z&ske=2026-02-03T10%3A50%3A55Z&sks=b&skv=2018-11-09&sig=mxNN8UbxdxFntobmmUu%2BDfS79zA9FDuhvy%2FuDdqOM0E%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc3MDExNTU3NywibmJmIjoxNzcwMTEzNzc3LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.XSRoh_QaJfcrgOLz6O-M8G9Zy8mBSCyHgFuI7cZMPXQ&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64&response-content-type=application%2Foctet-stream [following]\n",
            "--2026-02-03 10:16:17--  https://release-assets.githubusercontent.com/github-production-release-asset/106867604/31ade8d6-2fcd-4925-9218-5534d27a01dc?sp=r&sv=2018-11-09&sr=b&spr=https&se=2026-02-03T10%3A50%3A55Z&rscd=attachment%3B+filename%3Dcloudflared-linux-amd64&rsct=application%2Foctet-stream&skoid=96c2d410-5711-43a1-aedd-ab1947aa7ab0&sktid=398a6654-997b-47e9-b12b-9515b896b4de&skt=2026-02-03T09%3A50%3A17Z&ske=2026-02-03T10%3A50%3A55Z&sks=b&skv=2018-11-09&sig=mxNN8UbxdxFntobmmUu%2BDfS79zA9FDuhvy%2FuDdqOM0E%3D&jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmVsZWFzZS1hc3NldHMuZ2l0aHVidXNlcmNvbnRlbnQuY29tIiwia2V5Ijoia2V5MSIsImV4cCI6MTc3MDExNTU3NywibmJmIjoxNzcwMTEzNzc3LCJwYXRoIjoicmVsZWFzZWFzc2V0cHJvZHVjdGlvbi5ibG9iLmNvcmUud2luZG93cy5uZXQifQ.XSRoh_QaJfcrgOLz6O-M8G9Zy8mBSCyHgFuI7cZMPXQ&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64&response-content-type=application%2Foctet-stream\n",
            "Resolving release-assets.githubusercontent.com (release-assets.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.108.133, ...\n",
            "Connecting to release-assets.githubusercontent.com (release-assets.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 41260428 (39M) [application/octet-stream]\n",
            "Saving to: ‚Äòcloudflared-linux-amd64‚Äô\n",
            "\n",
            "cloudflared-linux-a 100%[===================>]  39.35M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2026-02-03 10:16:17 (415 MB/s) - ‚Äòcloudflared-linux-amd64‚Äô saved [41260428/41260428]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!chmod +x cloudflared-linux-amd64\n"
      ],
      "metadata": {
        "id": "-nBazNCQSq0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xttB06tnSvBx",
        "outputId": "445da6c3-38d5-49f2-b273-eb99cb69887a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "app.py\t       cloudflared-linux-amd64\tlogs.txt     train.py\n",
            "bert_data.pth  intents.json\t\tsample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!./cloudflared-linux-amd64 tunnel --url http://localhost:8501\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "obTjbRdxSWk4",
        "outputId": "73478c15-b2c2-4c3e-b548-e2ef4e706c8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[90m2026-02-03T10:16:31Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2026-02-03T10:16:31Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\u001b[90m2026-02-03T10:16:37Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2026-02-03T10:16:37Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2026-02-03T10:16:37Z\u001b[0m \u001b[32mINF\u001b[0m |  https://spots-journalist-warner-variety.trycloudflare.com                                 |\n",
            "\u001b[90m2026-02-03T10:16:37Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2026-02-03T10:16:37Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2026-02-03T10:16:37Z\u001b[0m \u001b[32mINF\u001b[0m Version 2026.1.2 (Checksum e157c54e929cc289cbd53860453168c2fe3439eb55e2e965a56579252585d9c1)\n",
            "\u001b[90m2026-02-03T10:16:37Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.24.11, GoArch: amd64\n",
            "\u001b[90m2026-02-03T10:16:37Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 protocol:quic url:http://localhost:8501]\n",
            "\u001b[90m2026-02-03T10:16:37Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update when run from the shell. To enable auto-updates, run cloudflared as a service: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/configure-tunnels/local-management/as-a-service/\n",
            "\u001b[90m2026-02-03T10:16:37Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: 26637ed7-e9df-4ffa-8c83-bc455b6fd0a4\n",
            "\u001b[90m2026-02-03T10:16:37Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2026-02-03T10:16:37Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2026-02-03T10:16:37Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2026-02-03T10:16:37Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2026-02-03T10:16:37Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2026-02-03T10:16:37Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2026-02-03T10:16:37Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.37\n",
            "2026/02/03 10:16:37 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2026-02-03T10:16:37Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0m1efdbdf2-4c44-488d-b915-462304de4654 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.37 \u001b[36mlocation=\u001b[0msin19 \u001b[36mprotocol=\u001b[0mquic\n",
            "\u001b[90m2026-02-03T10:33:21Z\u001b[0m \u001b[32mINF\u001b[0m Initiating graceful shutdown due to signal interrupt ...\n",
            "\u001b[90m2026-02-03T10:33:21Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m failed to run the datagram handler \u001b[31merror=\u001b[0m\u001b[31m\"context canceled\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.37\n",
            "\u001b[90m2026-02-03T10:33:21Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m failed to serve tunnel connection \u001b[31merror=\u001b[0m\u001b[31m\"accept stream listener encountered a failure while serving\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.37\n",
            "\u001b[90m2026-02-03T10:33:21Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Serve tunnel error \u001b[31merror=\u001b[0m\u001b[31m\"accept stream listener encountered a failure while serving\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.37\n",
            "\u001b[90m2026-02-03T10:33:21Z\u001b[0m \u001b[32mINF\u001b[0m Retrying connection in up to 1s \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.37\n",
            "\u001b[90m2026-02-03T10:33:21Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Connection terminated \u001b[36mconnIndex=\u001b[0m0\n",
            "\u001b[90m2026-02-03T10:33:21Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m no more connections active and exiting\n",
            "\u001b[90m2026-02-03T10:33:21Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel server stopped\n",
            "\u001b[90m2026-02-03T10:33:21Z\u001b[0m \u001b[32mINF\u001b[0m Metrics server stopped\n"
          ]
        }
      ]
    }
  ]
}