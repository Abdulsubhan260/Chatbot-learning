{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIX8fAF6_JqV",
        "outputId": "47b063bc-05b8-4e63-d287-23767e19f14f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization:"
      ],
      "metadata": {
        "id": "-mMw1tLCSTAv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"my name is ali. i am a student of university. i want to learn programming.\"\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokens=word_tokenize(text)\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jiai_4HJHJ9W",
        "outputId": "a8065843-79b3-493a-90de-2e04283bda5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['my', 'name', 'is', 'ali', '.', 'i', 'am', 'a', 'student', 'of', 'university', '.', 'i', 'want', 'to', 'learn', 'programming', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "tokenized=sent_tokenize(text)\n",
        "print(tokenized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REDnntAqIbXc",
        "outputId": "d1cae0de-b968-424a-81d9-4a4bb97d86a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['my name is ali.', 'i am a student of university.', 'i want to learn programming.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokens=word_tokenize(text)\n",
        "freq=FreqDist(tokens)\n",
        "print(freq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pUxw4n7mIzxs",
        "outputId": "6c820381-4e00-4916-edc1-869541f71ca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<FreqDist with 15 samples and 18 outcomes>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Removal of stoping words:"
      ],
      "metadata": {
        "id": "4P1XUbqYSYix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"my name is ali. i am a student of university. i want to learn programming.\"\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "tokenize_word=word_tokenize(text)\n",
        "\n",
        "tokenize_word_without_stop_words=[]\n",
        "for word in tokenize_word:\n",
        "    if word not in stop_words:\n",
        "        tokenize_word_without_stop_words.append(word)\n",
        "print(tokenize_word_without_stop_words)\n",
        "\n",
        "\n",
        "# print(set(tokenize_word)-set(tokenize_word_without_stop_words))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ueQUOlwDL3NP",
        "outputId": "c4c6c662-91a2-4a20-aaaa-60707a5e4570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['name', 'ali', '.', 'student', 'university', '.', 'want', 'learn', 'programming', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization:"
      ],
      "metadata": {
        "id": "vWjdQKUFSeTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word.lower()) for word in tokenize_word_without_stop_words]\n",
        "print(lemmatized_words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIFi0jCDObvv",
        "outputId": "7b94eccd-ccca-48d4-d59f-80898a894c9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['name', 'ali', '.', 'student', 'university', '.', 'want', 'learn', 'programming', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag of words(BOW):"
      ],
      "metadata": {
        "id": "Pbj268cNSiDy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "corpus = [\n",
        "    \"Hello, how are you?\",\n",
        "    \"Hi there, how is it going?\",\n",
        "    \"Good morning!\"\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "print(vectorizer.get_feature_names_out())\n",
        "print(X.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFQsP6faSH8A",
        "outputId": "7d4629a5-9ea6-448e-ee37-094d0e55ff2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['are' 'going' 'good' 'hello' 'hi' 'how' 'is' 'it' 'morning' 'there' 'you']\n",
            "[[1 0 0 1 0 1 0 0 0 0 1]\n",
            " [0 1 0 0 1 1 1 1 0 1 0]\n",
            " [0 0 1 0 0 0 0 0 1 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [\n",
        "    \"i am a student and learning about chatbots and NLP\",\n",
        "    \"chatbots are useful in many applications\",\n",
        "    \"learning NLP is fun and interesting\"\n",
        "]\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "X_bow = vectorizer.fit_transform(corpus)\n",
        "print(\"\\nVocabulary:\", vectorizer.get_feature_names_out())\n",
        "print(\"BoW Array:\\n\", X_bow.toarray())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CNZuCQVgSNJ4",
        "outputId": "955eb3b4-eef8-4002-a819-c18d341c341d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Vocabulary: ['about' 'am' 'and' 'applications' 'are' 'chatbots' 'fun' 'in'\n",
            " 'interesting' 'is' 'learning' 'many' 'nlp' 'student' 'useful']\n",
            "BoW Array:\n",
            " [[1 1 2 0 0 1 0 0 0 0 1 0 1 1 0]\n",
            " [0 0 0 1 1 1 0 1 0 0 0 1 0 0 1]\n",
            " [0 0 1 0 0 0 1 0 1 1 1 0 1 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "X_tfidf = tfidf.fit_transform(corpus)\n",
        "\n",
        "print(tfidf.get_feature_names_out())\n",
        "print(X_tfidf.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDcgsDkQS3ko",
        "outputId": "347b2fe9-38cd-4f8c-c687-004d208d38c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['about' 'am' 'and' 'applications' 'are' 'chatbots' 'fun' 'in'\n",
            " 'interesting' 'is' 'learning' 'many' 'nlp' 'student' 'useful']\n",
            "[[0.37665395 0.37665395 0.57291007 0.         0.         0.28645504\n",
            "  0.         0.         0.         0.         0.28645504 0.\n",
            "  0.28645504 0.37665395 0.        ]\n",
            " [0.         0.         0.         0.42339448 0.42339448 0.32200242\n",
            "  0.         0.42339448 0.         0.         0.         0.42339448\n",
            "  0.         0.         0.42339448]\n",
            " [0.         0.         0.34949812 0.         0.         0.\n",
            "  0.45954803 0.         0.45954803 0.45954803 0.34949812 0.\n",
            "  0.34949812 0.         0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# importing intents.json and performing nlp basic functions such as tokenization,lemmatization ,stemming and bag of words on it"
      ],
      "metadata": {
        "id": "jUlC_rA5Wq-u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_44LTPCVrs7",
        "outputId": "48c97fea-aa00-43fa-a07d-9828ca2f008b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open(\"intents.json\", \"r\") as f:\n",
        "    intents = json.load(f)\n",
        "\n",
        "\n",
        "print(\"Loaded intents:\")\n",
        "print(json.dumps(intents, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8O-uWwTbVvzF",
        "outputId": "8850cfb9-59c4-4fc5-e91a-48cbbbf89706"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded intents:\n",
            "{\n",
            "  \"intents\": [\n",
            "    {\n",
            "      \"tag\": \"greeting\",\n",
            "      \"patterns\": [\n",
            "        \"Hi\",\n",
            "        \"Hello\",\n",
            "        \"Hey\",\n",
            "        \"Assalamualaikum\",\n",
            "        \"What's up?\"\n",
            "      ],\n",
            "      \"responses\": [\n",
            "        \"Hello! How can I help you today?\",\n",
            "        \"Hi there, what can I do for you?\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"tag\": \"goodbye\",\n",
            "      \"patterns\": [\n",
            "        \"Bye\",\n",
            "        \"See you\",\n",
            "        \"Goodbye\",\n",
            "        \"Khuda hafiz\"\n",
            "      ],\n",
            "      \"responses\": [\n",
            "        \"Goodbye! Have a nice day \\ud83d\\ude0a\",\n",
            "        \"Khuda hafiz! Take care.\"\n",
            "      ]\n",
            "    },\n",
            "    {\n",
            "      \"tag\": \"thanks\",\n",
            "      \"patterns\": [\n",
            "        \"Thanks\",\n",
            "        \"Thank you\",\n",
            "        \"Shukriya\"\n",
            "      ],\n",
            "      \"responses\": [\n",
            "        \"You're welcome!\",\n",
            "        \"Happy to help!\"\n",
            "      ]\n",
            "    }\n",
            "  ]\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(sentence):\n",
        "    return nltk.word_tokenize(sentence)\n",
        "\n",
        "def stem(word):\n",
        "    return stemmer.stem(word.lower())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6OE0RfGzVyfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "basically,this code tokeize the intents.json and categorize each intent from where they belong.for example,hi,hello,hey belongs to greeting intent,similarlt bye,good bye,good night belongs good bye .\n",
        "after learning these patterns chatbot answer the user based on the intents in which questions of user belongs."
      ],
      "metadata": {
        "id": "p7Ol82PkaEWV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = []\n",
        "tags = []\n",
        "xy = []\n",
        "\n",
        "for intent in intents['intents']:\n",
        "    tag = intent['tag']\n",
        "    tags.append(tag)\n",
        "    for pattern in intent['patterns']:\n",
        "        w = tokenize(pattern)\n",
        "        all_words.extend(w)\n",
        "        xy.append((w, tag))\n",
        "\n",
        "\n",
        "print(\"Tokenized patterns:\")\n",
        "for pattern, tag in xy:\n",
        "    print(pattern ,\"->\", tag)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMPIqEkwV75p",
        "outputId": "99ae44d7-f52c-4805-cdf6-3af654385181"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized patterns:\n",
            "['Hi'] -> greeting\n",
            "['Hello'] -> greeting\n",
            "['Hey'] -> greeting\n",
            "['Assalamualaikum'] -> greeting\n",
            "['What', \"'s\", 'up', '?'] -> greeting\n",
            "['Bye'] -> goodbye\n",
            "['See', 'you'] -> goodbye\n",
            "['Goodbye'] -> goodbye\n",
            "['Khuda', 'hafiz'] -> goodbye\n",
            "['Thanks'] -> thanks\n",
            "['Thank', 'you'] -> thanks\n",
            "['Shukriya'] -> thanks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code cleans and organizes the words and tags. It removes punctuation like( ?, !, ., ,) from the list of all words then stems each word to its root form a\n",
        "\n",
        "after that it removes duplicates and sorts the words and tags"
      ],
      "metadata": {
        "id": "O4Vhe2JwcJ_h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ignore_words = ['?', '!', '.', ',']\n",
        "\n",
        "\n",
        "all_words = [stem(w) for w in all_words if w not in ignore_words]\n",
        "all_words = sorted(set(all_words))\n",
        "tags = sorted(set(tags))\n",
        "\n",
        "print(\"Cleaned Vocabulary:\", all_words)\n",
        "print(\"Tags:\", tags)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zveJ0c-8WBNm",
        "outputId": "906f7643-472f-4364-94ac-fcae29e28017"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Vocabulary: [\"'s\", 'assalamualaikum', 'bye', 'goodby', 'hafiz', 'hello', 'hey', 'hi', 'khuda', 'see', 'shukriya', 'thank', 'up', 'what', 'you']\n",
            "Tags: ['goodbye', 'greeting', 'thanks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "After tokenizing the patterns, the next step is stemming and creating a bag-of words\n",
        "\n",
        "stemming reduces each word to its root form so that similar words are treated the same for example “running” to “run” and “morning” to “morn.” This helps the chatbot recognize different variations of the same word\n",
        "\n",
        " Once all words are stemmed and cleaned (removing punctuation and duplicates) we create a bag-of-words for each pattern. A bag-of-words is a numeric vector representing the presence or absence of each word in the vocabulary for that sentence For example if a sentence contains a word from the vocabulary, the corresponding position in the vector is 1 otherwise it is 0"
      ],
      "metadata": {
        "id": "EdeyRRS9bQOV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bag_of_words(tokenized_sentence, all_words):\n",
        "\n",
        "    tokenized_sentence = [stem(w) for w in tokenized_sentence]\n",
        "\n",
        "\n",
        "    bag = np.zeros(len(all_words), dtype=np.float32)\n",
        "\n",
        "\n",
        "    for idx, w in enumerate(all_words):\n",
        "        if w in tokenized_sentence:\n",
        "            bag[idx] = 1\n",
        "    return bag\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EWEjfXZ9WHFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = []\n",
        "y_train = []\n",
        "\n",
        "print(\"Bag-of-Words for each pattern:\")\n",
        "\n",
        "for (pattern_sentence, tag) in xy:\n",
        "    bow = bag_of_words(pattern_sentence, all_words)\n",
        "    X_train.append(bow)\n",
        "\n",
        "    label = tags.index(tag)\n",
        "    y_train.append(label)\n",
        "\n",
        "    print(f\"Pattern: {pattern_sentence}\")\n",
        "    print(f\"BoW: {bow}\")\n",
        "    print(f\"Tag: {tag} -> Label: {label}\")\n",
        "    print(\"-------------------------\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3MrhbLSWHB2",
        "outputId": "dd5c649b-60b5-4155-a3a0-90406bf27564"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag-of-Words for each pattern:\n",
            "Pattern: ['Hi']\n",
            "BoW: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Tag: greeting -> Label: 0\n",
            "-------------------------\n",
            "Pattern: ['Hello']\n",
            "BoW: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Tag: greeting -> Label: 0\n",
            "-------------------------\n",
            "Pattern: ['Hey']\n",
            "BoW: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Tag: greeting -> Label: 0\n",
            "-------------------------\n",
            "Pattern: ['Assalamualaikum']\n",
            "BoW: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Tag: greeting -> Label: 0\n",
            "-------------------------\n",
            "Pattern: ['What', \"'s\", 'up', '?']\n",
            "BoW: [0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Tag: greeting -> Label: 0\n",
            "-------------------------\n",
            "Pattern: ['Bye']\n",
            "BoW: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Tag: goodbye -> Label: 1\n",
            "-------------------------\n",
            "Pattern: ['See', 'you']\n",
            "BoW: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Tag: goodbye -> Label: 1\n",
            "-------------------------\n",
            "Pattern: ['Goodbye']\n",
            "BoW: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Tag: goodbye -> Label: 1\n",
            "-------------------------\n",
            "Pattern: ['Khuda', 'hafiz']\n",
            "BoW: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            "Tag: goodbye -> Label: 1\n",
            "-------------------------\n",
            "Pattern: ['Thanks']\n",
            "BoW: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Tag: thanks -> Label: 2\n",
            "-------------------------\n",
            "Pattern: ['Thank', 'you']\n",
            "BoW: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0.]\n",
            "Tag: thanks -> Label: 2\n",
            "-------------------------\n",
            "Pattern: ['Shukriya']\n",
            "BoW: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "Tag: thanks -> Label: 2\n",
            "-------------------------\n"
          ]
        }
      ]
    }
  ]
}