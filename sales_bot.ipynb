{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lets make a fast food sales man by using bert:\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WHef4-IvO5WR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import transformers\n"
      ],
      "metadata": {
        "id": "xjgMApInO_lK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train.py\n",
        "# training.py\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import json\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from transformers import BertTokenizer,BertModel\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "from torch.optim import AdamW\n",
        "with open('fastfoodintents1.json','r') as f:\n",
        "  intents=json.load(f)\n",
        "\n",
        "tags=[]\n",
        "for intent in intents['intents']:\n",
        "  tags.append(intent['tag'])\n",
        "tags=sorted(set(tags))\n",
        "\n",
        "input_ids=[]\n",
        "attention_mask=[]\n",
        "labels=[]\n",
        "\n",
        "for intent in intents['intents']:\n",
        "  for pattern in intent['patterns']:\n",
        "\n",
        "    encoded=tokenizer(\n",
        "        pattern,\n",
        "        add_special_tokens=True,\n",
        "        max_length=32,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "\n",
        "    )\n",
        "\n",
        "    input_ids.append(encoded['input_ids'][0])\n",
        "    attention_mask.append(encoded['attention_mask'][0])\n",
        "    label_ids=tags.index(intent['tag'])\n",
        "    labels.append(label_ids)\n",
        "\n",
        "input_ids=torch.stack(input_ids)\n",
        "attention_mask=torch.stack(attention_mask)\n",
        "labels=torch.tensor(labels)\n",
        "\n",
        "\n",
        "class ChatDataset(Dataset):\n",
        "  def __init__(self,encodings,mask,labels):\n",
        "    self.encodings=encodings\n",
        "    self.mask=mask\n",
        "    self.labels=labels\n",
        "\n",
        "  def __getitem__(self,idx):\n",
        "\n",
        "    return{\n",
        "        'input_ids':self.encodings[idx],\n",
        "        'attention_mask':self.mask[idx],\n",
        "        'labels':self.labels[idx]\n",
        "\n",
        "    }\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.labels)\n",
        "\n",
        "\n",
        "dataset=ChatDataset(input_ids,attention_mask,labels)\n",
        "train_loader=DataLoader(dataset,batch_size=32,shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "class Bert_Arch(nn.Module):\n",
        "  def __init__(self,output_dim):\n",
        "    super(Bert_Arch,self).__init__()\n",
        "\n",
        "    self.bert=BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    self.dropout=nn.Dropout(0.1)\n",
        "\n",
        "    self.fc=nn.Linear(768,output_dim)\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self,sent_id,mask):\n",
        "    output=self.bert(sent_id,attention_mask=mask)\n",
        "\n",
        "    cls_vector=output.pooler_output\n",
        "\n",
        "    x=self.fc(self.dropout(cls_vector))\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "output_dim=len(tags)\n",
        "model=Bert_Arch(output_dim)\n",
        "model=model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "optimizer=AdamW(model.parameters(),lr=2e-5)\n",
        "\n",
        "cross_entropy=nn.CrossEntropyLoss()\n",
        "\n",
        "epochs=60\n",
        "for epoch in range(epochs):\n",
        "  total_loss=0\n",
        "  for batch in train_loader:\n",
        "    sent_id=batch['input_ids'].to(device)\n",
        "    mask=batch['attention_mask'].to(device)\n",
        "    labels=batch['labels'].to(device)\n",
        "\n",
        "\n",
        "    model.zero_grad()\n",
        "    preds=model(sent_id,mask)\n",
        "    loss=cross_entropy(preds,labels)\n",
        "    loss.backward()\n",
        "    total_loss=total_loss+loss.item()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(),1.0)\n",
        "\n",
        "    optimizer.step()\n",
        "  avg_loss=total_loss/len(train_loader)\n",
        "\n",
        "  if (epoch+1)%10==0:\n",
        "    print(f'epoch {epoch+1}/{epochs} |loss {avg_loss:.4f} ')\n",
        "\n",
        "\n",
        "\n",
        "output_data = {\n",
        "    \"model_state\": model.state_dict(),\n",
        "    \"output_dim\": output_dim,\n",
        "    \"tags\": tags,\n",
        "    \"vocab_size\": len(tokenizer),\n",
        "    \"embed_dim\": 768,\n",
        "    \"hidden_size\": 768,\n",
        "    \"max_len\": 20\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "torch.save(output_data, \"bert_data.pth\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mT4_FiPWTpFk",
        "outputId": "a297804c-724c-4b75-e9e9-ab0ba20a3b3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IaBp81dcnfPB",
        "outputId": "0d96fc2c-f9fa-4657-f622-9162a4ef0122"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n",
            "tokenizer_config.json: 100% 48.0/48.0 [00:00<00:00, 176kB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 7.38MB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 2.77MB/s]\n",
            "config.json: 100% 570/570 [00:00<00:00, 3.08MB/s]\n",
            "model.safetensors: 100% 440M/440M [00:02<00:00, 177MB/s]\n",
            "Loading weights: 100% 199/199 [00:00<00:00, 1336.70it/s, Materializing param=pooler.dense.weight]\n",
            "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-uncased\n",
            "Key                                        | Status     |  | \n",
            "-------------------------------------------+------------+--+-\n",
            "cls.predictions.transform.dense.bias       | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "cls.predictions.transform.dense.weight     | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "cls.seq_relationship.weight                | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "cls.predictions.transform.LayerNorm.bias   | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "cls.predictions.transform.LayerNorm.weight | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "cls.predictions.bias                       | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "cls.seq_relationship.bias                  | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- \u001b[38;5;208mUNEXPECTED\u001b[0m\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
            "epoch 10/60 |loss 1.7743 \n",
            "epoch 20/60 |loss 0.9717 \n",
            "epoch 30/60 |loss 0.4149 \n",
            "epoch 40/60 |loss 0.1559 \n",
            "epoch 50/60 |loss 0.0642 \n",
            "epoch 60/60 |loss 0.0383 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile chat.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random\n",
        "import numpy as np\n",
        "import json\n",
        "from transformers import BertModel,BertTokenizer\n",
        "\n",
        "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "with open('fastfoodintents1.json','r') as f:\n",
        "  intents=json.load(f)\n",
        "\n",
        "File=torch.load('bert_data.pth')\n",
        "model_state=File['model_state']\n",
        "output_dim=File['output_dim']\n",
        "tags=File['tags']\n",
        "\n",
        "\n",
        "class Bert_Arch(nn.Module):\n",
        "  def __init__(self,output_dim):\n",
        "    super(Bert_Arch,self).__init__()\n",
        "\n",
        "    self.bert=BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    self.dropout=nn.Dropout(0.1)\n",
        "    self.fc=nn.Linear(768,output_dim)\n",
        "\n",
        "\n",
        "  def forward(self,sent_id,mask):\n",
        "    output=self.bert(sent_id,attention_mask=mask)\n",
        "\n",
        "    cls_vector=output.pooler_output\n",
        "\n",
        "    x=self.fc(self.dropout(cls_vector))\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "model=Bert_Arch(output_dim)\n",
        "model.load_state_dict(model_state)\n",
        "model=model.to(device)\n",
        "\n",
        "model.eval()\n",
        "\n",
        "\n",
        "\n",
        "tokenizer=BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "bot_name=\"AI\"\n",
        "while True:\n",
        "  sentence=input('you: ')\n",
        "  if sentence=='quit':\n",
        "    break\n",
        "  encoded=tokenizer(\n",
        "      sentence,\n",
        "      add_special_tokens=True,\n",
        "      max_length=20,\n",
        "      truncation=True,\n",
        "      padding='max_length',\n",
        "      return_tensors='pt'\n",
        "  )\n",
        "\n",
        "  ids=encoded['input_ids'].to(device)\n",
        "  mask=encoded['attention_mask'].to(device)\n",
        "\n",
        "  output=model(ids,mask)\n",
        "  _,pred=torch.max(output,dim=1)\n",
        "\n",
        "  tag=tags[pred.item()]\n",
        "\n",
        "  prob=torch.softmax(output,dim=1)\n",
        "\n",
        "  prob=prob[0][pred.item()]\n",
        "  if prob.item() > 0.50:\n",
        "    for intent in intents['intents']:\n",
        "      if tag == intent['tag']:\n",
        "        print(f'{bot_name}: {random.choice(intent[\"responses\"])}')\n",
        "\n",
        "        break\n",
        "  else:\n",
        "\n",
        "    print(f'{bot_name}: I do not understand... (Low confidence: {prob.item():.2f})')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6xwXEKPYuhC",
        "outputId": "ea249166-e1b8-4728-8f1e-30ea3aadc91f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing chat.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python chat.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-zXtx2lfMQR",
        "outputId": "0a2c50e6-43ce-40d2-cc01-142d458a790f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading weights: 100% 199/199 [00:00<00:00, 1391.32it/s, Materializing param=pooler.dense.weight]\n",
            "\u001b[1mBertModel LOAD REPORT\u001b[0m from: bert-base-uncased\n",
            "Key                                        | Status     |  | \n",
            "-------------------------------------------+------------+--+-\n",
            "cls.predictions.transform.LayerNorm.bias   | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "cls.predictions.bias                       | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "cls.predictions.transform.LayerNorm.weight | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "cls.seq_relationship.bias                  | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "cls.predictions.transform.dense.weight     | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "cls.seq_relationship.weight                | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "cls.predictions.transform.dense.bias       | \u001b[38;5;208mUNEXPECTED\u001b[0m |  | \n",
            "\n",
            "\u001b[3mNotes:\n",
            "- \u001b[38;5;208mUNEXPECTED\u001b[0m\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
            "you: hello\n",
            "AI: Hi there! Ready to grab a bite? I can show you the menu or take an order.\n",
            "you: show me the menu\n",
            "AI: We serve a variety of burgers, pizzas, crispy fries, and cold beverages. Would you like to see the full digital menu?\n",
            "you: i want to order a burger\n",
            "AI: I can definitely help with that! What items would you like to add to your order?\n",
            "you: when are you open?\n",
            "AI: Our fast food counter is open 24/7 for hotel guests. For walk-in customers, we are open from 8:00 AM to 11:00 PM.\n",
            "you: where is food court\n",
            "AI: Our fast food outlet is located on the Ground Floor, right next to the main lobby.\n",
            "you: are there any deals\n",
            "AI: We have a 'Guest Special' combo: 1 Burger, Fries, and a Soda for just 1000pkr! We also offer 10% off for first-time orders via the bot.\n",
            "you: can i cancel my order\n",
            "AI: If the kitchen hasn't started yet, I can cancel it. Please provide your order ID or room number immediately.\n",
            "you: food was great\n",
            "AI: Thank you for sharing. We value your feedback! Would you like me to log this for our manager or connect you to a human agent?\n",
            "you: how can i pay\n",
            "AI: We accept all major credit cards, cash, and digital wallets. If you are a hotel guest, we can also charge the order directly to your room bill.\n",
            "you: payment methods\n",
            "AI: We accept all major credit cards, cash, and digital wallets. If you are a hotel guest, we can also charge the order directly to your room bill.\n",
            "you: see you later\n",
            "AI: Goodbye! We hope to serve you again soon.\n",
            "you: any deals?\n",
            "AI: We have a 'Guest Special' combo: 1 Burger, Fries, and a Soda for just 1000pkr! We also offer 10% off for first-time orders via the bot.\n",
            "you: i want to complain\n",
            "AI: Thank you for sharing. We value your feedback! Would you like me to log this for our manager or connect you to a human agent?\n",
            "you: quit\n"
          ]
        }
      ]
    }
  ]
}